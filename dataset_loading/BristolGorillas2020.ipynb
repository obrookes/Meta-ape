{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bg20\n",
    "from bg20 import BristolGorilla2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "south-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sitting-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infrared-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../BristolGorillas2020/images/train/'\n",
    "data = '../preprocessing/bg2020_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "variable-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant transformations inc. converting images to tensors\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize((50,50))\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "choice-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increased-writing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_name</th>\n",
       "      <th>annotation_file</th>\n",
       "      <th>image_path</th>\n",
       "      <th>annotation_path</th>\n",
       "      <th>annotation_0</th>\n",
       "      <th>annotation_1</th>\n",
       "      <th>annotation_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ayana-1-img-1.jpg</td>\n",
       "      <td>ayana-1-img-1.txt</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>1 0.557292 0.212500 0.026042 0.036111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ayana-1-img-2.jpg</td>\n",
       "      <td>ayana-1-img-2.txt</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>1 0.914844 0.618981 0.167187 0.308333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ayana-1-img-3.jpg</td>\n",
       "      <td>ayana-1-img-3.txt</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>1 0.657031 0.431481 0.030729 0.079630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ayana-1-img-4.jpg</td>\n",
       "      <td>ayana-1-img-4.txt</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>1 0.473438 0.503241 0.023958 0.058333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ayana-1-img-5.jpg</td>\n",
       "      <td>ayana-1-img-5.txt</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>../../BristolGorillas2020/images/train/ayana/a...</td>\n",
       "      <td>1 0.575521 0.295370 0.044792 0.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          file_name    annotation_file  \\\n",
       "0           0  ayana-1-img-1.jpg  ayana-1-img-1.txt   \n",
       "1           1  ayana-1-img-2.jpg  ayana-1-img-2.txt   \n",
       "2           2  ayana-1-img-3.jpg  ayana-1-img-3.txt   \n",
       "3           3  ayana-1-img-4.jpg  ayana-1-img-4.txt   \n",
       "4           4  ayana-1-img-5.jpg  ayana-1-img-5.txt   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "1  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "2  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "3  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "4  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "\n",
       "                                     annotation_path  \\\n",
       "0  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "1  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "2  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "3  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "4  ../../BristolGorillas2020/images/train/ayana/a...   \n",
       "\n",
       "                            annotation_0 annotation_1 annotation_2  \n",
       "0  1 0.557292 0.212500 0.026042 0.036111          NaN          NaN  \n",
       "1  1 0.914844 0.618981 0.167187 0.308333          NaN          NaN  \n",
       "2  1 0.657031 0.431481 0.030729 0.079630          NaN          NaN  \n",
       "3  1 0.473438 0.503241 0.023958 0.058333          NaN          NaN  \n",
       "4  1 0.575521 0.295370 0.044792 0.083333          NaN          NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "perceived-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=['annotation_1','annotation_2'],inplace=True)\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "bristol_gorillas=BristolGorilla2020(path=path,dataset=dataset,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(dataset=bristol_gorillas,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img[1].permute(1, 2, 0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-senate",
   "metadata": {},
   "source": [
    "**Learn2learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tracked-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import learn2learn as l2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moral-forward",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-37931a36ed82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBristolGorilla2020\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mlearn2learn/data/meta_dataset.pyx\u001b[0m in \u001b[0;36mlearn2learn.data.meta_dataset.MetaDataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mlearn2learn/data/meta_dataset.pyx\u001b[0m in \u001b[0;36mlearn2learn.data.meta_dataset.MetaDataset.create_bookkeeping\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/meta_ape/dataset_loading/bg20.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mcropped_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcropped_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = l2l.data.MetaDataset(BristolGorilla2020(path=path,dataset=dataset,transform=transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-injury",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
